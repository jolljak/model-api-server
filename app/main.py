# app/main.py
import os
import traceback
import asyncio
from concurrent.futures import ThreadPoolExecutor
from typing import Optional, Literal, List, Dict, Any

from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from dotenv import load_dotenv
import torch

# ====== ìœ í‹¸ (ê³µí†µ) ======
from .utils import (
    ensure_tmp_copy,
    transcode_to_wav_mono16k,
    get_media_duration_sec,
    is_silent,
)
from .asr import transcribe_file
from .diarization_utils import build_speaker_map, to_rttm
from pyannote.audio import Pipeline

# ============== í™˜ê²½ì„¤ì • ==============
load_dotenv()
os.environ.setdefault("OMP_NUM_THREADS", "1")
os.environ.setdefault("KMP_DUPLICATE_LIB_OK", "TRUE")

# ============== GPU ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€ í•¨ìˆ˜ ==============
def get_device() -> torch.device:
    """CUDAê°€ ê°€ëŠ¥í•˜ë©´ GPUë¡œ ê°•ì œ"""
    if os.getenv("FORCE_CPU") == "1":
        return torch.device("cpu")
    return torch.device("cuda" if torch.cuda.is_available() else "cpu")


# ============== FastAPI ì•± ==============
app = FastAPI(title="Mina ASR + Diarization (Study Mode)")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============== Pyannote ì¤€ë¹„ ==============
HF_TOKEN = os.getenv("HF_TOKEN")
if not HF_TOKEN:
    raise RuntimeError("í™˜ê²½ë³€ìˆ˜ HF_TOKENì´ ì„¤ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤ (Hugging Face í† í°).")
PIPELINE_NAME = "pyannote/speaker-diarization-3.1"
print(f"[INFO] Loading Pyannote pipeline: {PIPELINE_NAME}")
pipeline = Pipeline.from_pretrained(PIPELINE_NAME, use_auth_token=HF_TOKEN)
pipeline.to(get_device())
print("[INFO] Pyannote loaded successfully.")

# ============== í™”ì êµ¬ê°„ ì¶”ì¶œ í•¨ìˆ˜ ==============
def diarize_core(audio_path: str, min_speakers=None, max_speakers=None) -> Dict[str, Any]:
    """Pyannoteë¡œ í™”ì êµ¬ê°„ ë¶„ì„"""
    kwargs = {}
    if min_speakers:
        kwargs["min_speakers"] = int(min_speakers)
    if max_speakers:
        kwargs["max_speakers"] = int(max_speakers)

    diarization = pipeline({"audio": audio_path}, **kwargs)
    raw_labels = [label for _, _, label in diarization.itertracks(yield_label=True)]
    spk_map = build_speaker_map(raw_labels)

    segments = []
    for turn, _, spk in diarization.itertracks(yield_label=True):
        segments.append({
            "start": round(float(turn.start), 3),
            "end": round(float(turn.end), 3),
            "speaker": spk_map[spk],
        })

    return {
        "num_speakers": len(set([s["speaker"] for s in segments])),
        "segments": segments,
        "rttm": to_rttm(segments),
    }


# ============== STT + Diar í†µí•© ë¡œì§ ==============
def _overlap(a_start, a_end, b_start, b_end):
    return max(0.0, min(a_end, b_end) - max(a_start, b_start))

def assign_speakers(asr_segments: List[dict], diar_segments: List[dict]) -> List[dict]:
    """Whisper êµ¬ê°„ê³¼ Pyannote êµ¬ê°„ì„ ì‹œê°„ ê²¹ì¹¨ ê¸°ì¤€ìœ¼ë¡œ ë§¤ì¹­"""
    result = []
    for seg in asr_segments:
        a_start, a_end = float(seg.get("start", 0)), float(seg.get("end", 0))
        best_spk, best_overlap = None, 0.0
        for d in diar_segments:
            ov = _overlap(a_start, a_end, d["start"], d["end"])
            if ov > best_overlap:
                best_overlap, best_spk = ov, d["speaker"]

        result.append({
            "start": a_start,
            "end": a_end,
            "speaker": best_spk or "UNK",
            "text": seg.get("text", "")
        })
    return result


# ============== ë©”ì¸ ì—”ë“œí¬ì¸íŠ¸ ==============
@app.post("/transcribe-diarize")
async def transcribe_diarize(
    file: UploadFile = File(...),
    language: str = Form("auto"),
    task: Literal["transcribe", "translate"] = Form("transcribe"),
    min_speakers: Optional[int] = Form(None),
    max_speakers: Optional[int] = Form(None),
    parallel: bool = Form(True),  # ë³‘ë ¬ ì‹¤í–‰ ì—¬ë¶€ (GPU ì—¬ìœ  ì—†ìœ¼ë©´ Falseë¡œ ë³´ëƒ„)
):
    tmp_path = wav_path = None
    try:
        print("ğŸ”¹ ìŒì„±íŒŒì¼ ë¶„ì„ ì‹œì‘")

        # íŒŒì¼ ì €ì¥
        data = await file.read()
        if not data:
            raise HTTPException(status_code=400, detail="ë¹ˆ íŒŒì¼ì…ë‹ˆë‹¤.")
        tmp_path = ensure_tmp_copy(None, data, os.path.splitext(file.filename)[1])

        # WAV ë³€í™˜
        ok, wav_path, log = transcode_to_wav_mono16k(tmp_path)
        if not ok:
            raise HTTPException(status_code=415, detail="ffmpeg ë³€í™˜ ì‹¤íŒ¨")

        # ìœ íš¨ì„± ê²€ì‚¬
        if get_media_duration_sec(wav_path) <= 0.5:
            raise HTTPException(status_code=400, detail="ì˜¤ë””ì˜¤ ë„ˆë¬´ ì§§ìŒ")
        if is_silent(wav_path):
            raise HTTPException(status_code=400, detail="ë¬´ìŒ ì˜¤ë””ì˜¤")

        # GPU ê¸°ë°˜ ì‹¤í–‰ (STT + Diarization)
        device = get_device()
        loop = asyncio.get_event_loop()

        if parallel and device.type == "cuda":
            # GPU ì—¬ìœ  ìˆì„ ë•Œ ë³‘ë ¬ ì‹¤í–‰
            executor = ThreadPoolExecutor(max_workers=2)
            stt_future = loop.run_in_executor(executor, lambda: transcribe_file(wav_path, language, task))
            diar_future = loop.run_in_executor(executor, lambda: diarize_core(wav_path, min_speakers, max_speakers))
            stt_result, diar_result = await asyncio.gather(stt_future, diar_future)
        else:
            # GPU ë©”ëª¨ë¦¬ ì•„ë‚„ ë•Œ ìˆœì°¨ ì‹¤í–‰
            print("[INFO] Running sequentially to reduce GPU contention.")
            stt_result = transcribe_file(wav_path, language, task)
            diar_result = diarize_core(wav_path, min_speakers, max_speakers)

        # ë§¤í•‘
        combined = assign_speakers(stt_result["segments"], diar_result["segments"])

        print("ë¶„ì„ ì™„ë£Œ (GPU ëª¨ë“œ)")
        return {
            "ok": True,
            "device": str(device),
            "language": stt_result.get("language"),
            "duration": stt_result.get("duration"),
            "speaker_count": diar_result["num_speakers"],
            "segments": combined,
            "full_text": stt_result.get("text"),
        }

    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"GPU ëª¨ë“œ ì˜¤ë¥˜: {e}")

    finally:
        for p in (tmp_path, wav_path):
            try:
                if p and os.path.exists(p):
                    os.remove(p)
            except:
                pass


# ============== ê°„ë‹¨ í—¬ìŠ¤ì²´í¬ ==============
@app.get("/healthz")
def health():
    return {"ok": True, "device": str(get_device()), "cuda_available": torch.cuda.is_available()}
